{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2  # OpenCV for image processing\n",
    "import numpy as np  # NumPy for numerical operations\n",
    "import yaml  # YAML parsing\n",
    "import matplotlib.pyplot as plt  # For plotting results\n",
    "import os  # For file system operations\n",
    "import re  # For regular expressions\n",
    "from mpl_toolkits.mplot3d import Axes3D  # For 3D plotting\n",
    "import open3d as o3d  # Open3D for point cloud operations and ICP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom YAML constructor for OpenCV matrices (used to read matrices from YAML files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def opencv_matrix_constructor(loader, node):\n",
    "    mapping = loader.construct_mapping(node, deep=True)\n",
    "    rows = mapping['rows']\n",
    "    cols = mapping['cols']\n",
    "    data = mapping['data']\n",
    "    return np.array(data, dtype=np.float32).reshape(rows, cols)\n",
    "\n",
    "# Register the custom constructor to handle opencv-matrix in YAML files\n",
    "yaml.add_constructor('tag:yaml.org,2002:opencv-matrix', opencv_matrix_constructor, Loader=yaml.SafeLoader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load ground truth data (rotation and translation matrices) from a YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yaml(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            content = file.read()\n",
    "            # Remove the YAML version directive from the content\n",
    "            content = '\\n'.join([line for line in content.splitlines() if not line.startswith('%YAML:1.0')])\n",
    "            yaml_content = yaml.safe_load(content)\n",
    "            \n",
    "            # Validate if required fields are present in the YAML content\n",
    "            if not yaml_content or not all(key in yaml_content for key in [\"object_rotation_wrt_camera\", \"object_translation_wrt_camera\"]):\n",
    "                print(f\"❌ Error: Invalid YAML file {file_path}\")\n",
    "                return None, None\n",
    "\n",
    "            # Extract the rotation and translation matrices\n",
    "            R_gt = yaml_content[\"object_rotation_wrt_camera\"]\n",
    "            T_gt = np.array(yaml_content[\"object_translation_wrt_camera\"], dtype=np.float32).reshape(3, 1)\n",
    "\n",
    "            # Validate the rotation matrix\n",
    "            if np.any(np.isnan(T_gt)) or np.any(np.isinf(T_gt)) or not is_valid_rotation_matrix(R_gt):\n",
    "                print(f\"❌ Error: Invalid ground truth data in {file_path}\")\n",
    "                return None, None\n",
    "\n",
    "            print(f\"✅ Successfully loaded: {file_path}\")\n",
    "            return R_gt, T_gt\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading ground truth for {file_path}: {str(e)}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 3D object models (in .obj format) from the specified directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_obj_files(obj_model_dir, max_objects=25):\n",
    "    object_points_all = []\n",
    "    for obj_file in [f for f in os.listdir(obj_model_dir) if f.endswith('.obj')][:max_objects]:\n",
    "        try:\n",
    "            # Read vertices from the .obj file\n",
    "            vertices = [list(map(float, line.strip().split()[1:4])) for line in open(os.path.join(obj_model_dir, obj_file)) if line.startswith('v ')]\n",
    "            if vertices:\n",
    "                object_name = os.path.splitext(obj_file)[0]  # Extract object name without the .obj extension\n",
    "                object_points_all.append((np.array(vertices, dtype=np.float32), object_name))\n",
    "                print(f\"✅ Loaded {obj_file} with {len(vertices)} vertices as '{object_name}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {obj_file}: {str(e)}\")\n",
    "    return object_points_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute descriptors for the 3D object model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_object_descriptors(object_points, camera_matrix, dist_coeffs):\n",
    "    descriptors_all = []\n",
    "    for angle in range(0, 360, 30):  # Generate descriptors from multiple viewpoints (every 30 degrees)\n",
    "        R, _ = cv2.Rodrigues(np.radians(angle) * np.array([0, 1, 0]))  # Rotation around the Y-axis\n",
    "        image_points, _ = cv2.projectPoints(object_points, R, np.zeros((3, 1)), camera_matrix, dist_coeffs)\n",
    "        rendered_image = np.zeros((480, 640), dtype=np.uint8)\n",
    "        for (x, y) in image_points.reshape(-1, 2):\n",
    "            if 0 <= x < 640 and 0 <= y < 480:\n",
    "                cv2.circle(rendered_image, (int(x), int(y)), 3, 255, -1)\n",
    "        sift = cv2.SIFT_create(nfeatures=10000, contrastThreshold=0.01, edgeThreshold=10)\n",
    "        keypoints, descriptors = sift.detectAndCompute(rendered_image, None)\n",
    "        if descriptors is not None:\n",
    "            descriptors_all.extend(descriptors)\n",
    "    return np.array(descriptors_all)[:len(object_points)]  # Return only as many descriptors as there are object points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function to check if a rotation matrix is valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_rotation_matrix(R):\n",
    "    if R is None or R.shape != (3, 3):\n",
    "        return False\n",
    "    det_valid = np.isclose(np.linalg.det(R), 1.0, atol=1e-6)  # Ensure determinant is close to 1\n",
    "    ortho_valid = np.linalg.norm(R @ R.T - np.eye(3)) < 1e-6  # Ensure orthogonality\n",
    "    return det_valid and ortho_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the rotational error and translation error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to compute the rotational error between 2 rotation matrices \n",
    "def rotation_error(R_est, R_gt):\n",
    "    if R_est is None or R_gt is None or not is_valid_rotation_matrix(R_est) or not is_valid_rotation_matrix(R_gt):\n",
    "        print(\"❌ Error: Invalid rotation matrices.\")\n",
    "        return None\n",
    "    cos_theta = (np.trace(np.dot(R_est, R_gt.T)) - 1) / 2  # Compute cosine of the angle between the two rotation matrices\n",
    "    return np.degrees(np.arccos(np.clip(cos_theta, -1.0, 1.0)))  # Convert to degrees\n",
    "\n",
    "# Function to compute the translational error between two translation vectors\n",
    "def translation_error(T_est, T_gt):\n",
    "    if T_est is None or T_gt is None or np.any(np.isnan(T_est)) or np.any(np.isinf(T_est)) or np.any(np.isnan(T_gt)) or np.any(np.isinf(T_gt)):\n",
    "        print(\"❌ Error: Invalid translation vectors.\")\n",
    "        return None\n",
    "    return np.linalg.norm(T_est.flatten() - T_gt.flatten())  # Return the L2 norm of the difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply a mask to the image and depth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mask_to_depth_and_image(image, depth, mask):\n",
    "    image_masked = cv2.bitwise_and(image, image, mask=mask)  # Apply mask to the image\n",
    "    depth_masked = cv2.bitwise_and(depth, depth, mask=mask)  # Apply mask to the depth\n",
    "    return image_masked, depth_masked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize 3D object points in the image (for debugging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_points(image, image_points, object_points_for_pnp):\n",
    "    object_points_for_pnp = object_points_for_pnp.reshape(-1, 3)  # Flatten the 3D object points to a 2D array (N, 3)\n",
    "    assert object_points_for_pnp.shape[1] == 3, f\"Expected 3D object points, got shape {object_points_for_pnp.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to enhance image contrast using CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "def enhance_contrast(image):\n",
    "    if len(image.shape) == 3:\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  # Convert to grayscale if the image is in color\n",
    "    else:\n",
    "        gray = image  # Use the image as is if it's already grayscale\n",
    "    \n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))  # Create CLAHE object\n",
    "    enhanced = clahe.apply(gray)  # Apply CLAHE to enhance contrast\n",
    "    return enhanced\n",
    "\n",
    "# Function to reduce noise in the image using Gaussian Blur\n",
    "def reduce_noise(image):\n",
    "    blurred = cv2.GaussianBlur(image, (5, 5), 0)  # Apply Gaussian blur to reduce noise\n",
    "    return blurred\n",
    "\n",
    "# Function to enhance edges in the image using Canny edge detection\n",
    "def enhance_edges(image):\n",
    "    edges = cv2.Canny(image, 100, 200)  # Apply Canny edge detection\n",
    "    return edges\n",
    "\n",
    "# Function to sharpen the image using a convolution kernel\n",
    "def sharpen_image(image):\n",
    "    kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])  # Define sharpening kernel\n",
    "    sharpened = cv2.filter2D(image, -1, kernel)  # Apply the kernel to sharpen the image\n",
    "    return sharpened\n",
    "\n",
    "# Function to refine the mask using morphological operations (open and close)\n",
    "def refine_mask(mask):\n",
    "    kernel = np.ones((5, 5), np.uint8)  # Define kernel for morphological operations\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel)  # Remove small holes and noise\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel)  # Fill gaps in the mask\n",
    "    return mask\n",
    "\n",
    "# Function to normalize the depth map\n",
    "def normalize_depth(depth):\n",
    "    depth_normalized = cv2.normalize(depth, None, 0, 255, cv2.NORM_MINMAX)  # Normalize depth to range [0, 255]\n",
    "    return depth_normalized\n",
    "\n",
    "# Function to resize the image to the given dimensions\n",
    "def resize_image(image, width, height):\n",
    "    resized = cv2.resize(image, (width, height), interpolation=cv2.INTER_AREA)  # Resize using area interpolation\n",
    "    return resized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to preprocess image, mask, and depth for further processing\n",
    "def preprocess_image(image, mask, depth):\n",
    "    image = enhance_contrast(image)  # Enhance image contrast\n",
    "    image = reduce_noise(image)  # Reduce noise in the image\n",
    "    image = sharpen_image(image)  # Sharpen the image\n",
    "    mask = refine_mask(mask)  # Refine the mask\n",
    "    depth = normalize_depth(depth)  # Normalize the depth map\n",
    "    return image, mask, depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate object pose (rotation and translation) using the PnP algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_pose_with_pnp(image, object_points, camera_matrix, dist_coeffs, mask, depth=None):\n",
    "    image, mask, depth = preprocess_image(image, mask, depth)  # Preprocess the image, mask, and depth\n",
    "    image_masked, depth_masked = apply_mask_to_depth_and_image(image, depth, mask)  # Apply the mask to image and depth\n",
    "\n",
    "    image_points = []\n",
    "    object_points_for_pnp = []\n",
    "    good_matches = []\n",
    "\n",
    "    # Use SIFT to extract keypoints and descriptors\n",
    "    sift = cv2.SIFT_create(nfeatures=10000, contrastThreshold=0.01, edgeThreshold=10)\n",
    "    keypoints_image, descriptors_image = sift.detectAndCompute(image_masked, None)\n",
    "\n",
    "    if descriptors_image is None or len(keypoints_image) == 0:\n",
    "        print(f\"❌ No keypoints found in the image. {len(keypoints_image)}\")\n",
    "        return None, None\n",
    "\n",
    "    # Compute object descriptors (projecting object points to 2D from multiple viewpoints)\n",
    "    object_descriptors = compute_object_descriptors(object_points, camera_matrix, dist_coeffs)\n",
    "    if object_descriptors is None:\n",
    "        print(\"❌ Error: Failed to compute object descriptors.\")\n",
    "        return None, None\n",
    "\n",
    "    # Perform FLANN-based matching of keypoints between the image and object descriptors\n",
    "    index_params = dict(algorithm=1, trees=10)\n",
    "    search_params = dict(checks=100)\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    matches = flann.knnMatch(descriptors_image, object_descriptors, k=2)\n",
    "    good_matches = [m for m, n in matches if m.distance < 0.95 * n.distance]\n",
    "\n",
    "    # Ensure there are enough good matches for pose estimation\n",
    "    if len(good_matches) < 6:\n",
    "        print(f\"❌ Not enough valid matches found: {len(good_matches)} (need at least 6)\")\n",
    "        return None, None\n",
    "\n",
    "    # Extract image points and corresponding object points for PnP\n",
    "    for match in good_matches:\n",
    "        if match.trainIdx < len(object_points):  # Ensure that the index is within bounds\n",
    "            x, y = keypoints_image[match.queryIdx].pt\n",
    "            if mask[int(y), int(x)] > 0:  # Ensure the keypoint is within the masked region\n",
    "                image_points.append([x, y])\n",
    "                object_points_for_pnp.append(object_points[match.trainIdx])\n",
    "        else:\n",
    "            print(f\"❌ Invalid match: trainIdx {match.trainIdx} is out of bounds (max index {len(object_points) - 1})\")\n",
    "\n",
    "    # If there are fewer than 4 valid points, pose estimation cannot proceed\n",
    "    if len(image_points) < 4 or len(object_points_for_pnp) < 4:\n",
    "        print(f\"❌ Not enough valid points for PnP: {len(image_points)}\")\n",
    "        return None, None\n",
    "\n",
    "    image_points = np.array(image_points)\n",
    "    object_points_for_pnp = np.array(object_points_for_pnp)\n",
    "\n",
    "    # Perform PnP to estimate object pose\n",
    "    retval, rvec, tvec = cv2.solvePnP(object_points_for_pnp, image_points, camera_matrix, dist_coeffs)\n",
    "\n",
    "    if not retval:\n",
    "        print(\"❌ PnP pose estimation failed.\")\n",
    "        return None, None\n",
    "\n",
    "    R_est, _ = cv2.Rodrigues(rvec)  # Convert the rotation vector to a rotation matrix\n",
    "\n",
    "    return R_est, tvec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to find matching mask and depth files based on the base name of the query image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_matching_files(base_name, directory):\n",
    "    # List all files in the directory that start with the base name and contain 'mask' and 'depth' respectively\n",
    "    mask_files = [f for f in os.listdir(directory) if f.startswith(base_name) and 'mask' in f]\n",
    "    depth_files = [f for f in os.listdir(directory) if f.startswith(base_name) and 'depth' in f]\n",
    "    return mask_files, depth_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function to process images and compute pose estimation errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_images(image_dir, obj_model_dir, camera_matrix, dist_coeffs):\n",
    "    # Load 3D object points from .obj files (up to 10 objects)\n",
    "    object_points_all = load_obj_files(obj_model_dir, max_objects=10)\n",
    "    if not object_points_all:\n",
    "        print(\"❌ No valid OBJ files found.\")\n",
    "        return\n",
    "\n",
    "    # List query images (only '.png' files with 'image' in their name)\n",
    "    query_images = [f for f in os.listdir(image_dir) if f.endswith('.png') and 'image' in f]\n",
    "    if not query_images:\n",
    "        print(f\"❌ No query images found in {image_dir}.\")\n",
    "        return\n",
    "\n",
    "    # List YAML files that contain ground truth data for pose estimation\n",
    "    yml_files = [f for f in os.listdir(image_dir) if f.endswith('.yml')]\n",
    "    if not yml_files:\n",
    "        print(f\"❌ No YAML files found in {image_dir}.\")\n",
    "        return\n",
    "\n",
    "    # Initialize storage for errors (translation, rotation, and clutter levels) for each object\n",
    "    object_errors = {i: {\"trans_errors\": [], \"rot_errors\": [], \"clutter_levels\": []} for i in range(len(object_points_all))}\n",
    "\n",
    "    # Process each query image one by one\n",
    "    for query_image_file in query_images:\n",
    "        query_image_path = os.path.join(image_dir, query_image_file)\n",
    "        query_image = cv2.imread(query_image_path)\n",
    "\n",
    "        # Check if the query image was successfully read\n",
    "        if query_image is None:\n",
    "            print(f\"❌ Error reading query image: {query_image_file}\")\n",
    "            continue\n",
    "\n",
    "        # Extract base name of the query image for matching mask, depth, and ground truth\n",
    "        base_name = re.sub(r'-image-.*', '', query_image_file)\n",
    "        object_name = base_name\n",
    "\n",
    "        # Find corresponding object points based on the base name (match object model)\n",
    "        for idx, (points, name) in enumerate(object_points_all):\n",
    "            if name == object_name:\n",
    "                object_points = points\n",
    "                obj_idx = idx\n",
    "                break\n",
    "        else:\n",
    "            print(f\"❌ No matching object model found for {object_name}\")\n",
    "            continue\n",
    "\n",
    "        # Find corresponding mask and depth files based on the base name\n",
    "        mask_files, depth_files = find_matching_files(base_name, image_dir)\n",
    "        if not mask_files or not depth_files:\n",
    "            print(f\"❌ No matching mask or depth files found for {query_image_file}\")\n",
    "            continue\n",
    "\n",
    "        # Load the mask and depth images\n",
    "        mask_image_path = os.path.join(image_dir, mask_files[0])\n",
    "        depth_image_path = os.path.join(image_dir, depth_files[0])\n",
    "        mask = cv2.imread(mask_image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        depth = cv2.imread(depth_image_path, cv2.IMREAD_ANYDEPTH)\n",
    "\n",
    "        # Check if mask and depth images were successfully loaded\n",
    "        if mask is None or depth is None:\n",
    "            print(f\"❌ Error reading mask or depth image for {query_image_file}\")\n",
    "            continue\n",
    "\n",
    "        # Find the corresponding YAML file for the query image\n",
    "        matching_yml_file = next((yml_file for yml_file in yml_files if base_name in yml_file), None)\n",
    "        if not matching_yml_file:\n",
    "            print(f\"❌ No matching YML file found for {query_image_file}\")\n",
    "            continue\n",
    "\n",
    "        # Load the ground truth pose (rotation and translation) from the YAML file\n",
    "        R_gt, T_gt = load_yaml(os.path.join(image_dir, matching_yml_file))\n",
    "        if R_gt is None or T_gt is None:\n",
    "            print(f\"❌ Error loading ground truth for {query_image_file}\")\n",
    "            continue\n",
    "\n",
    "        # Extract clutter level from the filename (index 4 corresponds to the clutter level in the filename)\n",
    "        clutter_level = int(query_image_file.split('-')[4])\n",
    "        if clutter_level == 1:\n",
    "            clutter_description = \"No Clutter\"\n",
    "        elif clutter_level == 2:\n",
    "            clutter_description = \"1 Clutter Item\"\n",
    "        elif clutter_level == 3:\n",
    "            clutter_description = \"2 Clutter Items\"\n",
    "        else:\n",
    "            print(f\"❌ Invalid clutter level in filename: {query_image_file}\")\n",
    "            continue\n",
    "\n",
    "        # Estimate the object pose using the query image, object points, camera matrix, and depth information\n",
    "        R_est, T_est = estimate_pose_with_pnp(query_image, object_points, camera_matrix, dist_coeffs, mask, depth)\n",
    "        if R_est is None or T_est is None:\n",
    "            print(f\"❌ Pose estimation failed for {query_image_file}\")\n",
    "            continue\n",
    "\n",
    "        # Calculate the translation and rotational errors between estimated and ground truth poses\n",
    "        l2_distance = translation_error(T_est, T_gt)\n",
    "        rot_error = rotation_error(R_est, R_gt)\n",
    "        if l2_distance is None or rot_error is None:\n",
    "            print(f\"❌ Error calculation failed for {query_image_file}\")\n",
    "            continue\n",
    "\n",
    "        # Skip the current image if the translational error exceeds 2 meters\n",
    "        if l2_distance > 2.0:\n",
    "            print(f\"❌ Skipping {query_image_file} due to high translational error: {l2_distance} meters\")\n",
    "            continue\n",
    "\n",
    "        # Store the calculated errors and clutter level for the corresponding object\n",
    "        object_errors[obj_idx][\"trans_errors\"].append(l2_distance)\n",
    "        object_errors[obj_idx][\"rot_errors\"].append(rot_error)\n",
    "        object_errors[obj_idx][\"clutter_levels\"].append(clutter_level)\n",
    "\n",
    "        # Output the result for the current image\n",
    "        print(f\"✅ Pose estimation for {query_image_file} with object {object_name} (Clutter Level: {clutter_description})\")\n",
    "        print(f\"L2 Translation Error (meters): {l2_distance}\")\n",
    "        print(f\"Rotational Error (degrees): {rot_error}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "    # Plot pose estimation errors for each object\n",
    "    for obj_idx, (object_points, object_name) in enumerate(object_points_all):\n",
    "        errors = object_errors[obj_idx]\n",
    "        if not errors[\"trans_errors\"]:\n",
    "            continue\n",
    "\n",
    "        # Create a scatter plot of translational and rotational errors for different clutter levels\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        markers = ['s', 'x', 'o']\n",
    "        colors = ['blue', 'red', 'green']\n",
    "        labels = ['No Clutter', '1 Clutter Item', '2 Clutter Items']\n",
    "\n",
    "        # Plot the data points based on clutter levels\n",
    "        for clutter_level in range(1, 4):\n",
    "            indices = [i for i, cl in enumerate(errors[\"clutter_levels\"]) if cl == clutter_level]\n",
    "            plt.scatter([errors[\"trans_errors\"][i] for i in indices], [errors[\"rot_errors\"][i] for i in indices],\n",
    "                        color=colors[clutter_level - 1], marker=markers[clutter_level - 1], label=labels[clutter_level - 1], s=50, alpha=0.8)\n",
    "\n",
    "        # Add labels and title to the plot\n",
    "        plt.xlabel(\"Translational Error (meters)\", fontsize=12)\n",
    "        plt.ylabel(\"Rotational Error (degrees)\", fontsize=12)\n",
    "        plt.title(f\"Pose Estimation Errors for {object_name}\", fontsize=14)\n",
    "        plt.legend(title=\"Clutter Conditions\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use above code now in an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dir = r\"rutgers_apc_dataset\\all_data\" # Directory that has all the dataset \n",
    "obj_model_dir = r\"rutgers_apc_models\\apc_main\\object_models\\tarball\" # Directory that has all the .obj files\n",
    "camera_matrix = np.array([[575.8157, 0, 319.5], [0, 575.8157, 239.5], [0, 0, 1]], dtype=np.float64)\n",
    "dist_coeffs = np.zeros((4, 1), dtype=np.float64)\n",
    "\n",
    "# Run the process_images function to process the dataset and visualize errors\n",
    "process_images(image_dir, obj_model_dir, camera_matrix, dist_coeffs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
